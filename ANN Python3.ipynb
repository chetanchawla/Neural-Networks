{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6   \\\n",
      "0     -0.446891 -0.013397  0.232645  2.156649  1.652923 -0.210531 -0.662227   \n",
      "1      0.243747 -1.144175 -0.622214 -0.661979 -0.373315  0.520313 -1.307954   \n",
      "2     -1.417799 -0.088833 -0.647181 -1.141497 -1.321100  0.686798 -1.045569   \n",
      "3      0.656784  0.381201 -1.039011  1.285315  0.808324  1.428519 -1.144367   \n",
      "4      1.098520  1.706769 -1.030370  2.001009  2.751551  1.202229 -0.941066   \n",
      "5     -0.035569  1.481734  1.693223  1.437399  0.822527 -1.344757 -2.833138   \n",
      "6     -0.458196  1.103694 -0.985771  0.038616  0.510555 -0.036480  2.104910   \n",
      "7      0.009687  0.695721 -1.295533  0.609140 -0.484269  0.083173 -1.850543   \n",
      "8      0.711168  0.642080 -0.809361  1.377562  1.364644  0.662236 -1.127384   \n",
      "9     -0.026433  0.536858  0.809480  0.883268  0.631330 -0.834701  0.292860   \n",
      "10    -0.205401  0.450749 -0.495106  0.314523  1.854108  0.604280 -0.703304   \n",
      "11     0.537284 -0.354807 -1.852824  1.767041  1.117425  1.535077  1.694452   \n",
      "12    -0.594033  0.867767  1.831261 -0.655795 -0.586431 -1.536208 -1.092747   \n",
      "13     0.109851 -0.173262 -0.441256  1.327312  1.163962  1.054626 -2.686217   \n",
      "14    -1.196271  0.510688 -1.197820  0.654845  2.701547  1.010144  0.404620   \n",
      "15    -0.397864 -0.633862 -1.552343  0.979284  0.300053  0.864992 -3.542676   \n",
      "16     0.583588 -1.017661 -1.393142  0.279761  1.232985 -1.231032  0.516804   \n",
      "17     2.559759  0.512479  0.525487  0.974616  2.560457  0.096493  0.001915   \n",
      "18     0.230283 -1.433001 -0.172546 -1.724200 -2.436631  1.570051 -0.197270   \n",
      "19     0.138319  1.176707  0.173788 -1.832750 -1.848458  0.435625  0.643899   \n",
      "20     0.218338  0.281303 -1.476696 -1.420841 -3.305564 -0.079726 -2.744197   \n",
      "21    -1.754761 -0.768050  0.032691  1.415196 -0.048804  0.403433  2.655155   \n",
      "22    -1.399803  0.737673  0.659540 -0.328979  0.951495  0.147102  1.661120   \n",
      "23    -2.045445  0.120144  0.954810  1.182833  0.875899  0.424512 -1.255743   \n",
      "24     1.545020 -1.398371 -0.050901  1.468514  0.396145 -0.810420  2.398561   \n",
      "25    -0.663488 -1.544304 -0.018975 -0.954826 -1.181971  0.145996  0.052408   \n",
      "26    -1.380377 -0.239874  0.087510 -0.827064  0.383332  2.002654  1.592411   \n",
      "27    -0.380103 -1.991799  0.594203  0.849366 -0.229696  0.326362  3.422704   \n",
      "28     0.875267  2.046765 -0.265925 -0.261449  1.398170  1.053850 -1.762422   \n",
      "29    -1.074144  0.236529 -1.227874 -1.163974 -0.189188  1.785648  1.966391   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19970 -1.079736 -1.171587  1.081516  2.116520  0.954608 -1.143985 -2.613387   \n",
      "19971  0.689948 -1.565873 -1.359123 -1.562717  0.585558  0.438639 -0.268350   \n",
      "19972 -0.125665 -1.542844  0.267518  0.670497  0.269474 -0.132184 -1.813863   \n",
      "19973 -1.397768 -0.059196 -1.535288  0.920034  0.970300  2.301509 -0.860258   \n",
      "19974  0.110560 -1.964165  0.099601  1.317091  0.117643 -1.342084  0.036370   \n",
      "19975 -0.696419  1.580684 -0.604674 -1.584558  0.161848  0.694867  1.748024   \n",
      "19976  0.500918 -0.253837 -0.525418 -2.463573 -3.257645 -1.102115 -0.198172   \n",
      "19977 -0.605073 -0.330941  0.069221 -0.059605 -0.877646 -0.148447 -1.658698   \n",
      "19978 -0.419122 -0.821065 -0.490226 -0.415601  0.489522 -0.396855  1.362664   \n",
      "19979 -0.968481 -0.477572  1.555462 -2.053014  1.793930  0.233382 -0.616022   \n",
      "19980 -0.007817  0.726840  0.665570 -0.408382  3.548033  1.186334 -0.302762   \n",
      "19981  0.376215 -0.357565 -1.500266  0.382710  1.699042  0.602255  0.562411   \n",
      "19982  0.652167  0.258025 -1.250687  1.413876  1.458731  0.074075  0.094068   \n",
      "19983 -1.191770  0.438620 -1.363559  1.431821  1.137721 -0.104789  0.599131   \n",
      "19984  0.979708  0.983772 -1.305442  0.394110  1.502401  2.235545 -0.270226   \n",
      "19985  1.103622 -1.336016  1.481081  1.450818 -0.957979 -0.392736  3.164357   \n",
      "19986  0.891602  0.684080  0.535126 -0.482452  0.972972 -0.483349  0.040770   \n",
      "19987  0.141903 -0.546058  0.080077 -0.054500  2.334473  2.414882  1.779577   \n",
      "19988 -1.047536 -1.393929 -1.200668 -0.669164 -1.443275  0.997075 -2.107312   \n",
      "19989  0.137983 -1.405870  0.144551  1.609271  0.958279  0.224188 -2.127882   \n",
      "19990  0.936489 -0.611966  0.334625  2.533036 -0.494450  0.029113 -1.261533   \n",
      "19991 -1.336398 -0.768255  0.523960  0.387022  1.843949  0.617536 -2.150858   \n",
      "19992  0.209250  0.658697 -0.163084  1.615144  1.018254 -1.190301 -2.734797   \n",
      "19993  0.333826 -1.021209  0.953556  1.102484  1.090906  2.102094  0.105689   \n",
      "19994  0.574960 -0.932350  0.289456  1.809797 -0.721386 -0.331608 -1.788285   \n",
      "19995  0.146324 -0.618555  0.547413  0.851301  0.838880 -1.248402  0.698199   \n",
      "19996 -2.227923 -0.327196  1.258849 -0.945254  3.925262  0.316686 -1.061159   \n",
      "19997 -0.800405  1.477997  0.090057 -0.973997 -0.204970  0.224539  0.302265   \n",
      "19998 -1.217344 -0.785401 -0.413781 -0.335324  0.181671 -1.685733 -0.437981   \n",
      "19999  1.601005 -0.890398 -1.425049  0.887501  0.167205 -0.395042 -0.512554   \n",
      "\n",
      "             7         8         9         10        11        12        13  \\\n",
      "0      0.705144  1.434684 -0.445750 -0.178501 -0.905503 -0.594099 -0.728586   \n",
      "1      1.639481 -2.838816  0.331636  0.100261 -1.173067 -0.079774  2.194899   \n",
      "2     -0.376153 -2.272447 -0.669733 -1.070732  1.091993  0.692293  0.937617   \n",
      "3     -0.609958  0.983676  0.060917  0.471647 -0.594279  2.104027 -1.113597   \n",
      "4      1.965282  2.979131  1.402483 -1.432987  0.400322 -0.839851 -1.580974   \n",
      "5     -0.066823  0.440819 -2.746223  0.396444  0.584890  0.529624 -1.389421   \n",
      "6     -0.715512  1.085053 -1.193323 -1.143065  0.026853 -0.514957  0.331103   \n",
      "7     -0.883635 -0.530712 -0.460567  0.942175 -2.722683  0.106789 -0.865201   \n",
      "8     -0.156356  1.344837 -0.575138  1.880842 -1.120094  0.562877 -1.024425   \n",
      "9     -0.655215  0.310259  0.818279  0.374261  0.101193 -0.150289  0.253978   \n",
      "10    -1.439418 -0.263356  0.823229  0.352069 -0.398366  0.498711  1.514361   \n",
      "11    -0.325750  3.223503 -0.272204  0.132248 -0.587744  0.061450 -1.903139   \n",
      "12    -0.098522 -0.070886  0.617409 -0.070831 -1.656369  0.724284 -0.914212   \n",
      "13    -1.643301 -0.008729 -0.203433 -2.171994  0.271723 -0.415310 -0.509065   \n",
      "14     0.155461  3.920768 -0.627312  0.591907  0.592980  2.319264 -1.801173   \n",
      "15    -0.277768  0.133753 -1.618566 -0.150941 -1.802308  0.416340 -1.817282   \n",
      "16    -2.089918  1.617412 -0.193500  0.834089 -0.069472 -0.339559 -0.461336   \n",
      "17    -1.873737  3.146984  0.647621  1.352189 -0.022298  2.264940 -1.322864   \n",
      "18     1.631288 -1.699219  0.486809  1.341111  1.608923 -1.697402 -0.161624   \n",
      "19     0.429992 -1.234534  1.468979 -0.263426  1.509981  0.486375  0.288592   \n",
      "20     0.298724 -2.013763  0.349872 -0.946152 -1.060781 -0.452417 -1.919138   \n",
      "21     1.760100  1.303041  1.366594  0.231747 -1.105426  2.146502 -0.286504   \n",
      "22    -0.719731  1.280065  2.010386  1.864857  1.560342  0.967242  0.322272   \n",
      "23     0.017745  1.037579 -0.835292  0.100913 -0.875843  0.014073 -1.157737   \n",
      "24     0.461014  2.691297 -0.057196 -0.382608 -0.086189  0.303864 -1.555057   \n",
      "25    -0.881979 -0.472402 -0.829089 -0.935608  1.582503  0.288145 -0.373373   \n",
      "26     0.207351 -0.002085 -1.400511  1.841938 -0.434722  1.504364  1.255207   \n",
      "27    -0.571223  2.175077  0.336081  0.490730  0.690460 -0.801931 -0.937461   \n",
      "28     0.493414 -1.209240  0.493036 -0.365659  1.795869  0.256289  1.680214   \n",
      "29    -1.277096  0.840509  0.734025 -0.342502 -0.948300 -0.911299  0.063658   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19970  0.648503  0.614749 -1.454950  0.334785 -0.365938 -0.724615 -1.435743   \n",
      "19971 -1.046652  0.330516 -0.457375 -0.771924 -0.350225  0.644600  0.207036   \n",
      "19972 -1.009677  0.361922  0.715593 -0.975879 -0.282626  0.471462 -1.171263   \n",
      "19973  1.039621  1.210600  0.748649 -0.609749  0.634358  1.611082 -1.029680   \n",
      "19974  0.096108  1.570505 -0.492873  0.043012  0.290095 -0.096722 -1.761365   \n",
      "19975 -2.126961  1.210346 -1.165355 -0.009510  0.447857  1.235345 -0.093545   \n",
      "19976 -1.020121 -4.428492 -0.882125  0.517828  1.976041  0.450351  2.199318   \n",
      "19977 -0.093437 -2.377201 -0.423772 -0.379564  0.178653 -0.670415  0.993425   \n",
      "19978 -0.899692  0.075371 -1.675509  0.924197  0.898346  0.943146  1.094983   \n",
      "19979 -1.293039 -0.755983 -0.528672 -0.004170 -1.629456  0.378262  2.338653   \n",
      "19980 -0.343656  2.800520  1.266714  0.053760  0.247006  0.405462 -0.062879   \n",
      "19981 -0.439755  2.418965  0.105691 -0.127571 -1.621539 -0.944826 -0.920296   \n",
      "19982  0.237708  1.489259  0.336341 -1.449001 -0.024185 -1.629318 -0.479708   \n",
      "19983  1.048127  3.682884 -1.075878  0.058049 -0.201297 -0.949339 -2.907474   \n",
      "19984  1.023332  0.304224 -1.485379  0.592429 -0.520064  0.757682  0.796250   \n",
      "19985  1.572571  0.890549 -0.463178  0.915744  0.283136 -0.003513 -0.375232   \n",
      "19986  0.061037  0.852093 -1.384081 -0.612681  0.112233  1.212579 -0.000215   \n",
      "19987 -0.438865  5.220528  0.490518 -0.555521  1.401165 -0.322745 -2.755231   \n",
      "19988  1.096080 -3.167641 -2.170245 -0.408950 -1.322236  0.625315  1.209418   \n",
      "19989 -0.917227  0.778569  0.292439  1.175386  0.824683 -2.326682 -1.300821   \n",
      "19990 -1.578103 -1.950569  0.682826 -0.521106  0.384149 -2.190799  0.744485   \n",
      "19991 -0.828123  0.591877 -1.811554  1.649833 -0.276229  0.709116 -0.180878   \n",
      "19992  0.027051  1.044887 -0.728222  0.168552  1.138454  0.728837 -1.851219   \n",
      "19993 -0.020951 -1.433459  0.038078 -0.337026  0.320099  1.765981  2.443696   \n",
      "19994 -0.209197 -1.509586 -1.330304  1.263213 -0.778886 -0.964321 -0.117442   \n",
      "19995 -0.545033  0.994348 -0.054620  0.581239 -1.842074 -0.552879 -0.102096   \n",
      "19996  1.846706  3.692157  0.698035 -0.184239 -0.546036 -0.457919 -1.030581   \n",
      "19997  0.361146 -1.906636  1.381875  0.034366  1.690199 -0.901569  2.183842   \n",
      "19998 -1.176626 -1.434436 -0.880986  2.117703 -2.136842 -1.185261  1.547704   \n",
      "19999  0.050777 -0.034945 -0.289219  0.639189  0.981751 -0.110296 -0.187214   \n",
      "\n",
      "             14        15        16        17        18        19  \n",
      "0     -0.627002 -0.240520  0.131696 -0.945878  1.840922 -0.279024  \n",
      "1      0.242839  0.412164 -0.328714  0.902931 -1.418728 -0.290139  \n",
      "2      0.262142  0.399238  0.614614  1.209465 -1.526386  0.337771  \n",
      "3     -0.188799  1.226253 -1.067763 -0.350275 -0.746426 -1.258696  \n",
      "4     -0.659587  2.292688  0.494188 -1.583187 -2.468254 -0.469648  \n",
      "5      0.441056 -0.543496  1.357878  0.064575 -0.114624 -0.824641  \n",
      "6      2.074059  1.216302 -0.258826 -0.823207  0.382628 -0.410283  \n",
      "7     -0.555448 -0.022804  0.582193  0.630541  1.026752 -1.831992  \n",
      "8     -0.889786 -1.858208  0.169443 -0.655441 -0.363746 -1.392009  \n",
      "9      0.834363  1.064346  0.300306 -0.443632  0.334967 -0.446605  \n",
      "10     0.414542 -1.620456 -0.435401 -0.641475  0.296072  0.566354  \n",
      "11     0.019829 -0.959503 -0.543963 -1.441771 -0.016614 -0.000030  \n",
      "12    -2.271849  0.901819  0.686790  0.538720  0.023600  1.658103  \n",
      "13     0.474916 -0.727192  0.696568 -0.039478  0.049809  1.240636  \n",
      "14     0.326933 -0.185388  0.387865 -1.885730 -0.311882 -1.215865  \n",
      "15    -0.950753  0.258916  0.813653  0.516958  1.152996 -0.927078  \n",
      "16    -1.205018 -0.467321  2.090767 -0.902767  0.036719  0.645032  \n",
      "17    -0.046648 -0.650866  0.887548 -1.648439  1.457507 -0.016622  \n",
      "18    -0.908634  0.539529  0.149469  1.473275 -1.049110  0.805711  \n",
      "19     0.961814 -0.216249 -1.403484  0.981795  1.731296 -0.257558  \n",
      "20    -0.527540  0.145166  0.508278  2.405068  0.596933  0.991635  \n",
      "21    -0.699177 -1.402451  0.697540 -0.824978 -1.735171 -1.224950  \n",
      "22    -0.782050  0.444464  1.768022 -0.925182 -1.185367  1.005437  \n",
      "23     1.566808 -0.093230 -0.249318 -0.357183 -0.903778 -0.983527  \n",
      "24    -1.315969  0.443337  1.805501 -1.174876 -0.920092 -0.653332  \n",
      "25    -0.818391 -0.179533 -0.199927  0.639965 -1.268488  0.134901  \n",
      "26    -0.820785 -0.986337 -0.063852 -0.438402 -0.220381  1.206368  \n",
      "27    -1.825955 -0.001370 -0.104297 -0.995555 -0.504960 -1.487144  \n",
      "28     0.088339 -1.128946 -0.634461 -0.041949 -0.236849 -0.235371  \n",
      "29     0.217042 -0.661118  2.485222 -0.369088 -0.257193  1.179544  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "19970 -1.612212 -0.968523  0.479747 -0.112471  0.306122 -1.337765  \n",
      "19971 -1.718463  0.725187  1.148330 -0.136542  0.465450 -0.239011  \n",
      "19972  0.474309  0.638894 -0.108959  0.158793 -0.255365 -0.846448  \n",
      "19973  0.326689  0.350164  0.595231 -0.487443  0.324375 -0.389751  \n",
      "19974  0.903677 -0.037132  0.914176 -0.385553  0.275847  0.410388  \n",
      "19975 -1.843739  0.172822  0.462167 -0.501149  0.344963 -0.401019  \n",
      "19976 -1.378474 -0.291328  0.338366  2.287937  1.573973 -0.245447  \n",
      "19977  0.907518 -0.080775  1.351038  1.083317  0.231676 -1.608497  \n",
      "19978  0.524546 -0.019707  0.353145 -0.477427 -0.732941  0.517392  \n",
      "19979  0.355415  1.422379 -1.402491 -0.392273  0.419680 -0.209522  \n",
      "19980 -0.949699 -0.070841 -0.232543 -1.863977 -1.082182  0.104043  \n",
      "19981  1.082571 -0.452641 -0.113285 -1.240951  1.016820 -0.588447  \n",
      "19982  0.539113 -0.269323  0.999741 -0.974258 -1.547140  0.639842  \n",
      "19983  1.557473 -0.234438  0.813307 -1.267367 -0.238719 -0.889355  \n",
      "19984 -1.063824 -1.706427 -0.176277 -0.668930 -0.820147 -1.140628  \n",
      "19985  0.473672  0.303933 -0.003872 -0.477539  0.090715 -0.538709  \n",
      "19986  1.194481  1.541643 -0.405503 -0.523110  1.406971  0.480685  \n",
      "19987 -0.919783 -0.724166  0.775850 -2.155207 -0.356815  0.827772  \n",
      "19988 -0.016920  0.017300 -1.193385  1.582788  0.152164  0.752602  \n",
      "19989  0.845008 -0.759443 -1.342698 -0.203363  0.000343  1.479930  \n",
      "19990 -0.064536 -0.364019  1.120931  0.587074 -0.435640 -1.600303  \n",
      "19991  0.189521 -0.260929  0.494317 -0.468408 -0.028467 -0.084159  \n",
      "19992 -0.557488 -0.540948 -1.093975 -0.143217  0.656618  0.335369  \n",
      "19993 -0.964389  1.708245 -0.722388 -0.360455  2.232539  0.191554  \n",
      "19994  1.423488  0.747174  0.240036  0.780817  1.003428  0.258658  \n",
      "19995  0.307538 -1.267832 -0.165875 -0.717397 -0.808588  0.796346  \n",
      "19996  2.406014 -0.978943 -0.086879 -1.964453 -0.875574  1.535930  \n",
      "19997  0.831008  0.036308  0.516088  0.378670  0.110968 -0.176363  \n",
      "19998  1.174712 -0.429117  0.132068  0.249277 -0.941496  1.661661  \n",
      "19999 -0.226811 -0.547772 -0.006706 -0.023773  1.220809  1.489330  \n",
      "\n",
      "[20000 rows x 20 columns]\n",
      "       0\n",
      "0      3\n",
      "1      2\n",
      "2      2\n",
      "3      3\n",
      "4      3\n",
      "5      3\n",
      "6      0\n",
      "7      3\n",
      "8      3\n",
      "9      0\n",
      "10     1\n",
      "11     0\n",
      "12     3\n",
      "13     1\n",
      "14     3\n",
      "15     3\n",
      "16     3\n",
      "17     3\n",
      "18     2\n",
      "19     2\n",
      "20     2\n",
      "21     0\n",
      "22     4\n",
      "23     3\n",
      "24     0\n",
      "25     3\n",
      "26     4\n",
      "27     0\n",
      "28     2\n",
      "29     3\n",
      "...   ..\n",
      "19970  3\n",
      "19971  2\n",
      "19972  3\n",
      "19973  0\n",
      "19974  0\n",
      "19975  3\n",
      "19976  2\n",
      "19977  2\n",
      "19978  4\n",
      "19979  4\n",
      "19980  1\n",
      "19981  0\n",
      "19982  3\n",
      "19983  0\n",
      "19984  1\n",
      "19985  0\n",
      "19986  2\n",
      "19987  0\n",
      "19988  2\n",
      "19989  3\n",
      "19990  1\n",
      "19991  1\n",
      "19992  3\n",
      "19993  1\n",
      "19994  1\n",
      "19995  3\n",
      "19996  1\n",
      "19997  2\n",
      "19998  2\n",
      "19999  0\n",
      "\n",
      "[20000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('C:\\\\Users\\\\asd\\\\Desktop\\\\Winter Training\\\\data1.csv', header=None)\n",
    "print(data)\n",
    "labels=pd.read_csv('C:\\\\Users\\\\asd\\\\Desktop\\\\Winter Training\\\\label1.csv',header=None)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 20)\n",
      "(20000, 1)\n"
     ]
    }
   ],
   "source": [
    "npData=np.asarray(data)\n",
    "npLabels=np.asarray(labels)\n",
    "print(npData.shape)\n",
    "print(npLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(npLabels))\n",
    "print(np.min(npLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(npLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(20000, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "num=np.max(npLabels)+1\n",
    "oneHot=np.eye(num)[npLabels]\n",
    "print(oneHot.ndim)\n",
    "print(oneHot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "oneHotRe=np.reshape(oneHot, [20000,5])\n",
    "print(oneHotRe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData,testDataDash=train_test_split(npData,train_size=0.8,test_size=0.2,shuffle=False)\n",
    "trainLabel,testLabelDash=train_test_split(oneHotRe,train_size=0.8,test_size=0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validData,testData=train_test_split(testDataDash,train_size=0.5,test_size=0.5,shuffle=False)\n",
    "validLabel,testLabel=train_test_split(testLabelDash,train_size=0.5,test_size=0.5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "h=int(input())#Number of neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "b1=np.zeros(h)\n",
    "b2=np.zeros(5)\n",
    "print(b1)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=np.random.normal(0, 1/np.sqrt(20), (20,h))\n",
    "w2=np.random.normal(0, 1/np.sqrt(h), (h,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actFunc(huehue,choice):\n",
    "    #tanh\n",
    "    if(choice==1):\n",
    "        return np.tanh(huehue)\n",
    "    elif(choice==2):\n",
    "        temp=np.exp(huehue)\n",
    "        return temp/np.sum(temp,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003\n"
     ]
    }
   ],
   "source": [
    "alpha = float(input()) #learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "batch=int(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunc(y,t):\n",
    "    return -(t*(np.log(y))+(1-t)*np.log(1-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwFunc(data):\n",
    "    a0=data\n",
    "    z1=np.dot(a0,w1)+b1\n",
    "    a1=ActFunc(z1,1)\n",
    "    z2=np.dot(a1,w2)+b2\n",
    "    a2=ActFunc(z2,2)\n",
    "    return a0,z1,a1,z2,a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Acc(y,t,size):\n",
    "    maxData=np.argmax(y,axis=1)\n",
    "    maxLabel=np.argmax(t,axis=1)\n",
    "    compare=np.equal(maxData,maxLabel)\n",
    "    count=np.sum(compare)\n",
    "    return (count/size)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs100\n"
     ]
    }
   ],
   "source": [
    "epochs=int(input(\"epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 0.858215972097\n",
      "Training accuracy 82.21875\n",
      "Training Cost 0.792296599463\n",
      "Training accuracy 84.25625\n",
      "Training Cost 0.751948802759\n",
      "Training accuracy 85.325\n",
      "Training Cost 0.713926627014\n",
      "Training accuracy 86.08125\n",
      "Training Cost 0.678456142742\n",
      "Training accuracy 86.7\n",
      "Training Cost 0.643325694733\n",
      "Training accuracy 87.41875\n",
      "Training Cost 0.611726280281\n",
      "Training accuracy 87.96875\n",
      "Training Cost 0.589149961747\n",
      "Training accuracy 88.33125\n",
      "Training Cost 0.573725007378\n",
      "Training accuracy 88.675\n",
      "Training Cost 0.556589227219\n",
      "Training accuracy 89.1625\n",
      "Training Cost 0.540892583892\n",
      "Training accuracy 89.56875\n",
      "Training Cost 0.525565219665\n",
      "Training accuracy 90.13125\n",
      "Training Cost 0.518333248565\n",
      "Training accuracy 90.4875\n",
      "Training Cost 0.558455335844\n",
      "Training accuracy 89.6125\n",
      "Training Cost 0.516390934632\n",
      "Training accuracy 90.5125\n",
      "Training Cost 0.508927241299\n",
      "Training accuracy 90.65625\n",
      "Training Cost 0.506988010411\n",
      "Training accuracy 90.675\n",
      "Training Cost 0.506504235021\n",
      "Training accuracy 90.7625\n",
      "Training Cost 0.504800288706\n",
      "Training accuracy 90.7875\n",
      "Training Cost 0.501807670544\n",
      "Training accuracy 90.85625\n",
      "Training Cost 0.499274482961\n",
      "Training accuracy 90.9375\n",
      "Training Cost 0.497599561274\n",
      "Training accuracy 90.9625\n",
      "Training Cost 0.496445892189\n",
      "Training accuracy 90.95\n",
      "Training Cost 0.495457820156\n",
      "Training accuracy 90.9875\n",
      "Training Cost 0.49451931591\n",
      "Training accuracy 90.975\n",
      "Training Cost 0.493696950148\n",
      "Training accuracy 91.025\n",
      "Training Cost 0.493068375246\n",
      "Training accuracy 91.0125\n",
      "Training Cost 0.492659032696\n",
      "Training accuracy 91.01875\n",
      "Training Cost 0.492460362242\n",
      "Training accuracy 91.03125\n",
      "Training Cost 0.492448404104\n",
      "Training accuracy 91.01875\n",
      "Training Cost 0.492582427962\n",
      "Training accuracy 91.05625\n",
      "Training Cost 0.492800857514\n",
      "Training accuracy 91.1125\n",
      "Training Cost 0.493034340588\n",
      "Training accuracy 91.11875\n",
      "Training Cost 0.493233089844\n",
      "Training accuracy 91.05625\n",
      "Training Cost 0.493382436229\n",
      "Training accuracy 91.08125\n",
      "Training Cost 0.493490368336\n",
      "Training accuracy 91.09375\n",
      "Training Cost 0.493561458364\n",
      "Training accuracy 91.10625\n",
      "Training Cost 0.493581175779\n",
      "Training accuracy 91.1375\n",
      "Training Cost 0.493519710323\n",
      "Training accuracy 91.1125\n",
      "Training Cost 0.493349581807\n",
      "Training accuracy 91.125\n",
      "Training Cost 0.493066052875\n",
      "Training accuracy 91.0875\n",
      "Training Cost 0.49270182541\n",
      "Training accuracy 91.10625\n",
      "Training Cost 0.492336455181\n",
      "Training accuracy 91.11875\n",
      "Training Cost 0.492112155688\n",
      "Training accuracy 91.13125\n",
      "Training Cost 0.492274369152\n",
      "Training accuracy 91.13125\n",
      "Training Cost 0.493226870982\n",
      "Training accuracy 91.125\n",
      "Training Cost 0.495320194785\n",
      "Training accuracy 91.06875\n",
      "Training Cost 0.497138168502\n",
      "Training accuracy 91.075\n",
      "Training Cost 0.493120935276\n",
      "Training accuracy 91.14375\n",
      "Training Cost 0.479563369061\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.473184971271\n",
      "Training accuracy 91.3375\n",
      "Training Cost 0.483115580757\n",
      "Training accuracy 90.91875\n",
      "Training Cost 0.469623382595\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.487371678289\n",
      "Training accuracy 91.1625\n",
      "Training Cost 0.506204117062\n",
      "Training accuracy 90.83125\n",
      "Training Cost 0.481299622192\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.47042415527\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.466435784573\n",
      "Training accuracy 91.69375\n",
      "Training Cost 0.465290528405\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.468964114133\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.472252578549\n",
      "Training accuracy 91.43125\n",
      "Training Cost 0.464527011017\n",
      "Training accuracy 91.7375\n",
      "Training Cost 0.538629827786\n",
      "Training accuracy 90.24375\n",
      "Training Cost 0.479903678714\n",
      "Training accuracy 91.4\n",
      "Training Cost 0.4693808098\n",
      "Training accuracy 91.6125\n",
      "Training Cost 0.470515138404\n",
      "Training accuracy 91.51875\n",
      "Training Cost 0.476798988809\n",
      "Training accuracy 91.49375\n",
      "Training Cost 0.481326479619\n",
      "Training accuracy 91.26875\n",
      "Training Cost 0.476428006937\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.470817797813\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.46858234352\n",
      "Training accuracy 91.575\n",
      "Training Cost 0.469852084878\n",
      "Training accuracy 91.575\n",
      "Training Cost 0.476363570685\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.484583945588\n",
      "Training accuracy 91.175\n",
      "Training Cost 0.478067020156\n",
      "Training accuracy 91.38125\n",
      "Training Cost 0.467360665213\n",
      "Training accuracy 91.6375\n",
      "Training Cost 0.462049744854\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.460991617518\n",
      "Training accuracy 91.75625\n",
      "Training Cost 0.460713211061\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46710573111\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.513126077672\n",
      "Training accuracy 90.65625\n",
      "Training Cost 0.479580798946\n",
      "Training accuracy 91.29375\n",
      "Training Cost 0.46488654192\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.462697008664\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.464531424702\n",
      "Training accuracy 91.71875\n",
      "Training Cost 0.472118433713\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.482848367188\n",
      "Training accuracy 91.14375\n",
      "Training Cost 0.479197415223\n",
      "Training accuracy 91.30625\n",
      "Training Cost 0.469180132751\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.46365041105\n",
      "Training accuracy 91.775\n",
      "Training Cost 0.461775061237\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.462764697403\n",
      "Training accuracy 91.81875\n",
      "Training Cost 0.469466744567\n",
      "Training accuracy 91.65625\n",
      "Training Cost 0.486182358813\n",
      "Training accuracy 91.20625\n",
      "Training Cost 0.487070277765\n",
      "Training accuracy 91.18125\n",
      "Training Cost 0.469991324922\n",
      "Training accuracy 91.575\n",
      "Training Cost 0.461541703488\n",
      "Training accuracy 91.74375\n",
      "Training Cost 0.458857770087\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.459039785996\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.463582391477\n",
      "Training accuracy 91.7125\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for iteration in range(int(len(trainData)/batch)):\n",
    "        a0,z1,a1,z2,a2=forwFunc(trainData[iteration*batch:(iteration+1)*batch,:])\n",
    "        y=a2\n",
    "        labelBatch=trainLabel[iteration*batch:(iteration+1)*batch,:]\n",
    "        del2=(y-labelBatch)\n",
    "        del1=np.dot(del2,w2.T)*(1-pow(actFunc(z1,1),2))\n",
    "        dcdw2=np.dot(a1.T,del2)\n",
    "        dcdw1=np.dot(a0.T,del1)\n",
    "        dcdb1=np.sum(del1,axis=0)\n",
    "        dcdb2=np.sum(del2,axis=0)\n",
    "        w1=w1-alpha*dcdw1\n",
    "        w2=w2-alpha*dcdw2\n",
    "        b2=b2-alpha*dcdb2\n",
    "        b1=b1-alpha*dcdb1\n",
    "    a0,z1,a1,z2,a2=forwFunc(trainData)\n",
    "    print(\"Training Cost\",(np.sum(costFunc(a2,trainLabel)))/16000.0)\n",
    "    print(\"Training accuracy\",Acc(a2,trainLabel,len(trainLabel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
