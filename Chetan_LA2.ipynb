{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:/Users/asd/Desktop/Winter Training/data1.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we convert the data into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44689111 -0.01339702  0.23264493 ..., -0.94587809  1.84092189\n",
      "  -0.27902404]\n",
      " [ 0.24374694 -1.14417545 -0.62221358 ...,  0.90293107 -1.41872787\n",
      "  -0.29013935]\n",
      " [-1.41779879 -0.0888327  -0.647181   ...,  1.20946469 -1.52638614\n",
      "   0.33777109]\n",
      " ..., \n",
      " [-0.80040466  1.47799654  0.0900574  ...,  0.37866958  0.11096771\n",
      "  -0.17636265]\n",
      " [-1.21734449 -0.78540102 -0.41378073 ...,  0.24927734 -0.94149613\n",
      "   1.66166061]\n",
      " [ 1.60100517 -0.89039824 -1.42504908 ..., -0.02377265  1.2208092\n",
      "   1.48933029]]\n"
     ]
    }
   ],
   "source": [
    "npData=np.asarray(data)\n",
    "print npData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the dimensions, size and shape of the array of data. npData represents the whole data array of 20000X20 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "400000\n",
      "(20000L, 20L)\n"
     ]
    }
   ],
   "source": [
    "print npData.ndim\n",
    "print npData.size\n",
    "print npData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The shape here says that there are 20000 datapoints or inputs we can give to the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 here suggests that there are 20 different feutures of the data which we are studying and hence signifies the number of input neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Doing the same for labels.csv and saving the array of corresponding labels of the data 20000X1 into a numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label=pd.read_csv('C:/Users/asd/Desktop/Winter Training/label1.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [2]\n",
      " [2]\n",
      " ..., \n",
      " [2]\n",
      " [2]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "npLabel=np.asarray(label)\n",
    "print npLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "20000\n",
      "(20000L, 1L)\n"
     ]
    }
   ],
   "source": [
    "print npLabel.ndim\n",
    "print npLabel.size\n",
    "print npLabel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we convert our labels in the one hot format where we convert each labelinto a matrix so that all numbers in the label can be clasified individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "(20000L, 5L)\n"
     ]
    }
   ],
   "source": [
    "n_values = np.max(npLabel) + 1\n",
    "oneHotDash=np.eye(n_values)[npLabel]\n",
    "oneHot=np.reshape(oneHotDash,(20000,5))##we reshape from a 3D matrix as built by default using eye function to a 2D array\n",
    "print oneHot\n",
    "print oneHot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000L, 20L)\n",
      "(4000L, 20L)\n",
      "(16000L, 5L)\n",
      "[[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "(4000L, 5L)\n"
     ]
    }
   ],
   "source": [
    "trainData,testDataDash=train_test_split(npData,train_size=0.8,test_size=0.2,shuffle=False)#divide data in a 80:20 ratio\n",
    "trainLabel,testLabelDash=train_test_split(oneHot,train_size=0.8,test_size=0.2,shuffle=False)\n",
    "print trainData.shape\n",
    "print testDataDash.shape\n",
    "print trainLabel.shape\n",
    "print trainLabel\n",
    "print testLabelDash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000L, 20L)\n",
      "(2000L, 20L)\n",
      "(2000L, 5L)\n",
      "(2000L, 5L)\n"
     ]
    }
   ],
   "source": [
    "validateData, testData=train_test_split(testDataDash,train_size=0.5,test_size=0.5,shuffle=False)#dividing rest20% data half half in validation and testing data\n",
    "validateLabel, testLabel=train_test_split(testLabelDash,train_size=0.5,test_size=0.5,shuffle=False)\n",
    "print validateData.shape\n",
    "print testData.shape\n",
    "print validateLabel.shape\n",
    "print testLabel.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 20 neurons in the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 5 neurons in the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 10 neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we take input of the number of neurons in the hidden layer of our single layered neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "h=input()#number of neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biases are initialized to be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "b1=np.zeros(h)\n",
    "b2=np.zeros(5)\n",
    "print b1\n",
    "print b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We make randomized weight matricesaccording to random normal distribution which is according to 0 mean and 1/nL variance or1/sqrt(nL) standard deviation where nL if the number of inuts to that particular layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22360679775 0.03125\n",
      "[[-0.04500349 -0.30626745  0.46046042 ..., -0.27906925  0.0710719\n",
      "   0.18238645]\n",
      " [ 0.09609046  0.1882918   0.40261746 ...,  0.10857276  0.06954503\n",
      "   0.04317422]\n",
      " [-0.07735335  0.20140305  0.2416908  ...,  0.28440318 -0.01812375\n",
      "  -0.24110837]\n",
      " ..., \n",
      " [ 0.07977438 -0.17601851 -0.23602727 ..., -0.1316114   0.33085649\n",
      "   0.20932723]\n",
      " [-0.00600366  0.17764019  0.0464903  ...,  0.09134482  0.33533739\n",
      "   0.19153143]\n",
      " [-0.20104192 -0.32797319 -0.01290107 ...,  0.22132287  0.03630171\n",
      "  -0.15123682]]\n",
      "\n",
      "[[-0.00287981 -0.01793399 -0.01191854 -0.01831779 -0.01688113]\n",
      " [-0.03880488  0.00818628  0.01344801 -0.00275655  0.02041775]\n",
      " [-0.00659054  0.01696337  0.01073463  0.00408753  0.05804079]\n",
      " ..., \n",
      " [-0.05677376 -0.02745974  0.02481959 -0.07135361 -0.01040379]\n",
      " [ 0.02893276  0.01016503 -0.04219277  0.05354089 -0.02215447]\n",
      " [-0.00203969 -0.01804712 -0.00365559 -0.01449677 -0.01772363]]\n",
      "(20L, 1024L)\n",
      "(1024L, 5L)\n"
     ]
    }
   ],
   "source": [
    "std1=(1/np.sqrt(20))#standard deviation of initialiing first weights\n",
    "std2=(1/np.sqrt(h))#standard deviation of initialiing second weights\n",
    "print std1,std2\n",
    "w1=np.random.normal(0,std1,(20,h))#gaussian random assignment of weight\n",
    "print w1\n",
    "print''\n",
    "w2=np.random.normal(0,std2,(h,5))#gaussian random assignment of weights \n",
    "print w2\n",
    "print w1.shape\n",
    "print w2.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we make a function which takes in two parameters, haha is any matrix of mXn shape and choose is an integer variable which takes input of which type of activation do we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actFunc(haha,choose):\n",
    "    if choose==1:#Sigmoid\n",
    "        return (1/(1+np.exp(haha)))\n",
    "    elif choose==2:#ReLU\n",
    "        return np.max(0,haha)\n",
    "    elif choose==3:#tanh\n",
    "        return np.tanh(haha)\n",
    "    elif choose==4:#softmax\n",
    "        temp=np.exp(haha)\n",
    "        return temp/np.sum(temp,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "alpha = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter the size of the mini batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When batch size = 1, we are doing a stochastic approach of neural network which takes one datapoint at a time and adjust weight according to each one of them. The learning process is the slowest in stochastic as well as a very high accuracy is found out. Also, it requires much more processing power due to continuous forward and backward passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When batch size is equal to the whole dataset's size, we say that we have approached with a full batch approach. Here, we take the whole data at once, apply a forward pass on it, calculate the gradient descent(it gives a single line to the minima), and do the backward pass and hence update the weights and biases(parameters). This has the fastest learning rate and learns very good. However, with larger datasets, chances are that we might not be able to use them as arrays in our controllers. Hence, we use a trade-off between stochastic and full batch which is known as the mini batch process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When batch size is more than one and is less than the total number of data points, we are doing a mini batch approach wherein, we take a chunk of the whole data, say 1000 of 16000 images, at a time, do a forward pass on this chunk of data, compute the gradient and do a backward pass to adjust the weights and biases(parameters) according to these 1000 data points. This completes one iteration. Then, we take the next chunk of data with the same batch size ad repeat the same process. Hence, when we complete this on our complete dataset, we say that we have completed one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "batch=input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costFunc(y,t):\n",
    "    return -(t*(np.log(y))+(1-t)*np.log(1-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwardFunc(Data):\n",
    "    a0=Data\n",
    "    z1=np.dot(a0,w1)+b1\n",
    "    a1=actFunc(z1,3)\n",
    "    z2=np.dot(a1,w2)+b2\n",
    "    a2=actFunc(z2,4)\n",
    "    return a0,z1,a1,z2,a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracyFunc(output,labelData,dataSize):\n",
    "    maxInRow=np.argmax(output, axis=1)\n",
    "    maxInLabels=np.argmax(labelData, axis=1)\n",
    "    huehue=np.equal(maxInRow,maxInLabels)\n",
    "    count=np.sum(huehue)\n",
    "    #count=0\n",
    "    #for l in range(16000):\n",
    "    #    if maxInRow[l]==maxInLabels[l]:\n",
    "    #        count=count+1\n",
    "    return (count/dataSize)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.44689111 -0.01339702  0.23264493  2.15664884  1.65292329 -0.21053079\n",
      " -0.66222709  0.70514395  1.43468372 -0.4457503  -0.17850107 -0.90550337\n",
      " -0.59409921 -0.7285856  -0.62700244 -0.24051958  0.13169638 -0.94587809\n",
      "  1.84092189 -0.27902404]\n",
      "[ 0.  0.  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(trainData[0])\n",
    "print(trainLabel[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000L, 5L)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLabel[i*batch:(i+1)*batch,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000L, 5L)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost\n",
      "3.81827343515\n",
      "training accuracy\n",
      "60.21875\n",
      "cost\n",
      "2.40222174611\n",
      "training accuracy\n",
      "65.18125\n",
      "cost\n",
      "1.55049415451\n",
      "training accuracy\n",
      "70.23125\n",
      "cost\n",
      "1.63501759004\n",
      "training accuracy\n",
      "71.85625\n",
      "cost\n",
      "1.11134256805\n",
      "training accuracy\n",
      "78.75625\n",
      "cost\n",
      "1.10641824391\n",
      "training accuracy\n",
      "78.05625\n",
      "cost\n",
      "0.95758495661\n",
      "training accuracy\n",
      "81.0625\n",
      "cost\n",
      "0.968170985638\n",
      "training accuracy\n",
      "81.95\n",
      "cost\n",
      "0.788847322109\n",
      "training accuracy\n",
      "85.2375\n",
      "cost\n",
      "0.786331954738\n",
      "training accuracy\n",
      "85.29375\n",
      "cost\n",
      "0.731581371861\n",
      "training accuracy\n",
      "86.21875\n",
      "cost\n",
      "0.750164336519\n",
      "training accuracy\n",
      "85.375\n",
      "cost\n",
      "0.736396778174\n",
      "training accuracy\n",
      "85.7\n",
      "cost\n",
      "0.673129929402\n",
      "training accuracy\n",
      "87.40625\n",
      "cost\n",
      "0.646193876852\n",
      "training accuracy\n",
      "87.91875\n",
      "cost\n",
      "0.626979191083\n",
      "training accuracy\n",
      "88.25\n",
      "cost\n",
      "0.599228242014\n",
      "training accuracy\n",
      "88.7875\n",
      "cost\n",
      "0.692662828654\n",
      "training accuracy\n",
      "86.36875\n",
      "cost\n",
      "0.623715356508\n",
      "training accuracy\n",
      "87.9625\n",
      "cost\n",
      "0.585827894922\n",
      "training accuracy\n",
      "89.0\n",
      "cost\n",
      "0.576307250258\n",
      "training accuracy\n",
      "89.3875\n",
      "cost\n",
      "0.55405373582\n",
      "training accuracy\n",
      "89.775\n",
      "cost\n",
      "0.535726470819\n",
      "training accuracy\n",
      "90.05625\n",
      "cost\n",
      "0.512373656036\n",
      "training accuracy\n",
      "90.59375\n",
      "cost\n",
      "0.626614094231\n",
      "training accuracy\n",
      "87.7625\n",
      "cost\n",
      "0.568801985618\n",
      "training accuracy\n",
      "88.84375\n",
      "cost\n",
      "0.489999512147\n",
      "training accuracy\n",
      "90.74375\n",
      "cost\n",
      "0.515501968339\n",
      "training accuracy\n",
      "90.3625\n",
      "cost\n",
      "0.517464337958\n",
      "training accuracy\n",
      "90.50625\n",
      "cost\n",
      "0.48429771347\n",
      "training accuracy\n",
      "91.00625\n",
      "cost\n",
      "0.474123221922\n",
      "training accuracy\n",
      "91.19375\n",
      "cost\n",
      "0.468201996745\n",
      "training accuracy\n",
      "91.34375\n",
      "cost\n",
      "0.484123973473\n",
      "training accuracy\n",
      "91.13125\n",
      "cost\n",
      "0.492319017431\n",
      "training accuracy\n",
      "91.0\n",
      "cost\n",
      "0.438073813938\n",
      "training accuracy\n",
      "91.75\n",
      "cost\n",
      "0.546752589257\n",
      "training accuracy\n",
      "89.3375\n",
      "cost\n",
      "0.51226551913\n",
      "training accuracy\n",
      "89.91875\n",
      "cost\n",
      "0.498166626607\n",
      "training accuracy\n",
      "90.1625\n",
      "cost\n",
      "0.474798585946\n",
      "training accuracy\n",
      "90.725\n",
      "cost\n",
      "0.483747684486\n",
      "training accuracy\n",
      "90.48125\n",
      "cost\n",
      "0.439075492741\n",
      "training accuracy\n",
      "91.53125\n",
      "cost\n",
      "0.466631659622\n",
      "training accuracy\n",
      "90.925\n",
      "cost\n",
      "0.48073618557\n",
      "training accuracy\n",
      "90.65625\n",
      "cost\n",
      "0.476227861364\n",
      "training accuracy\n",
      "91.25625\n",
      "cost\n",
      "0.447302897784\n",
      "training accuracy\n",
      "91.8125\n",
      "cost\n",
      "0.419597175372\n",
      "training accuracy\n",
      "92.26875\n",
      "cost\n",
      "0.409882102788\n",
      "training accuracy\n",
      "92.45\n",
      "cost\n",
      "0.404617749467\n",
      "training accuracy\n",
      "92.55625\n",
      "cost\n",
      "0.407116784071\n",
      "training accuracy\n",
      "92.53125\n",
      "cost\n",
      "0.42371974767\n",
      "training accuracy\n",
      "92.2\n",
      "cost\n",
      "0.405027849185\n",
      "training accuracy\n",
      "92.65625\n",
      "cost\n",
      "0.400705204513\n",
      "training accuracy\n",
      "92.38125\n",
      "cost\n",
      "0.490266370132\n",
      "training accuracy\n",
      "90.4625\n",
      "cost\n",
      "0.442021536353\n",
      "training accuracy\n",
      "91.4375\n",
      "cost\n",
      "0.445347807908\n",
      "training accuracy\n",
      "91.375\n",
      "cost\n",
      "0.411949968437\n",
      "training accuracy\n",
      "92.25625\n",
      "cost\n",
      "0.399339218933\n",
      "training accuracy\n",
      "92.51875\n",
      "cost\n",
      "0.381573622199\n",
      "training accuracy\n",
      "92.9625\n",
      "cost\n",
      "0.428054583707\n",
      "training accuracy\n",
      "92.025\n",
      "cost\n",
      "0.400344369964\n",
      "training accuracy\n",
      "92.575\n",
      "cost\n",
      "0.365033831011\n",
      "training accuracy\n",
      "93.325\n",
      "cost\n",
      "0.346350814809\n",
      "training accuracy\n",
      "93.76875\n",
      "cost\n",
      "0.381234143327\n",
      "training accuracy\n",
      "92.8625\n",
      "cost\n",
      "0.46719365488\n",
      "training accuracy\n",
      "91.08125\n",
      "cost\n",
      "0.378956297118\n",
      "training accuracy\n",
      "92.925\n",
      "cost\n",
      "0.410479936806\n",
      "training accuracy\n",
      "92.16875\n",
      "cost\n",
      "0.396668416498\n",
      "training accuracy\n",
      "92.48125\n",
      "cost\n",
      "0.35841619579\n",
      "training accuracy\n",
      "93.375\n",
      "cost\n",
      "0.398625816136\n",
      "training accuracy\n",
      "92.5375\n",
      "cost\n",
      "0.391401735715\n",
      "training accuracy\n",
      "92.81875\n",
      "cost\n",
      "0.335295176259\n",
      "training accuracy\n",
      "93.8875\n",
      "cost\n",
      "0.319327570047\n",
      "training accuracy\n",
      "94.30625\n",
      "cost\n",
      "0.311143509484\n",
      "training accuracy\n",
      "94.475\n",
      "cost\n",
      "0.491314004898\n",
      "training accuracy\n",
      "90.73125\n",
      "cost\n",
      "0.35236266502\n",
      "training accuracy\n",
      "93.29375\n",
      "cost\n",
      "0.350522186565\n",
      "training accuracy\n",
      "93.41875\n",
      "cost\n",
      "0.355935691092\n",
      "training accuracy\n",
      "93.3125\n",
      "cost\n",
      "0.380579585656\n",
      "training accuracy\n",
      "92.96875\n",
      "cost\n",
      "0.360282440471\n",
      "training accuracy\n",
      "93.39375\n",
      "cost\n",
      "0.297500224546\n",
      "training accuracy\n",
      "94.75625\n",
      "cost\n",
      "0.318594624163\n",
      "training accuracy\n",
      "94.11875\n",
      "cost\n",
      "0.425146312404\n",
      "training accuracy\n",
      "91.88125\n",
      "cost\n",
      "0.302628034335\n",
      "training accuracy\n",
      "94.41875\n",
      "cost\n",
      "0.294648218491\n",
      "training accuracy\n",
      "94.575\n",
      "cost\n",
      "0.313299052106\n",
      "training accuracy\n",
      "94.2625\n",
      "cost\n",
      "0.458396214755\n",
      "training accuracy\n",
      "91.375\n",
      "cost\n",
      "0.358214640946\n",
      "training accuracy\n",
      "93.04375\n",
      "cost\n",
      "0.305617882929\n",
      "training accuracy\n",
      "94.4\n",
      "cost\n",
      "0.365728042857\n",
      "training accuracy\n",
      "93.3375\n",
      "cost\n",
      "0.281348395621\n",
      "training accuracy\n",
      "95.05625\n",
      "cost\n",
      "0.278909971655\n",
      "training accuracy\n",
      "95.00625\n",
      "cost\n",
      "0.30607859151\n",
      "training accuracy\n",
      "94.36875\n",
      "cost\n",
      "0.294361836571\n",
      "training accuracy\n",
      "94.59375\n",
      "cost\n",
      "0.282099039366\n",
      "training accuracy\n",
      "94.9125\n",
      "cost\n",
      "0.407170639476\n",
      "training accuracy\n",
      "92.125\n",
      "cost\n",
      "0.27791809461\n",
      "training accuracy\n",
      "94.95625\n",
      "cost\n",
      "0.296597887143\n",
      "training accuracy\n",
      "94.6125\n",
      "cost\n",
      "0.269780291648\n",
      "training accuracy\n",
      "95.1875\n",
      "cost\n",
      "0.303124267757\n",
      "training accuracy\n",
      "94.2625\n",
      "cost\n",
      "0.421451503884\n",
      "training accuracy\n",
      "91.95625\n",
      "validation accuracy\n",
      "85.95\n"
     ]
    }
   ],
   "source": [
    "for j in range (100):\n",
    "    #one epoch\n",
    "    for i in range(trainData.shape[0]/batch):#for each iteration, we use a for loop.\n",
    "        a0,z1,a1,z2,a2=forwardFunc(trainData[i*batch:(i+1)*batch,:])\n",
    "        y=a2\n",
    "        labelBatch=trainLabel[i*batch:(i+1)*batch,:]\n",
    "        #Backward pass\n",
    "        del2=(y-labelBatch)\n",
    "        del1=np.dot(del2,w2.T)*(1-pow(actFunc(z1,3),2))\n",
    "        dcdw2=np.dot(a1.T,del2)\n",
    "        dcdw1=np.dot(a0.T,del1)\n",
    "        dcdb1=np.sum(del1,axis=0)\n",
    "        dcdb2=np.sum(del2,axis=0)\n",
    "        w1=w1-alpha*dcdw1\n",
    "        w2=w2-alpha*dcdw2\n",
    "        b2=b2-alpha*dcdb2\n",
    "        b1=b1-alpha*dcdb1\n",
    "    #Cost after one epoch\n",
    "    a0,z1,a1,z2,a2=forwardFunc(trainData)\n",
    "    ytrue=a2\n",
    "    cost=np.sum(costFunc(ytrue,trainLabel))/16000\n",
    "    print \"cost\"\n",
    "    print cost\n",
    "    accuracy = accuracyFunc(ytrue,trainLabel,16000.0)\n",
    "    print \"training accuracy\"\n",
    "    print accuracy\n",
    "##validation\n",
    "va0,vz1,va1,vz2,va2=forwardFunc(validateData)\n",
    "vOutput=va2\n",
    "accuracy = accuracyFunc(vOutput,validateLabel,2000.0)\n",
    "print \"validation accuracy\"\n",
    "print accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
