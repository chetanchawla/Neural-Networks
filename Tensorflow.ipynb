{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow BasicsTutorial with MNIST DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a powerful library for doing large-scale numerical computation. One of the tasks at which it excels is implementing and training deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist1= fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(mnist1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forking MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two lines are used to fork the mnist data set in our code\n",
    "Here mnist is a class now, which stores the training, validation and testing sets as NumPy arrays.\n",
    "\n",
    "mnist data is available in tensorflow examples. It is read by input_data.read_data_sets function and is present as MNIST_data. Here, the labels are prespecified to be onehot arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-93d8da72a918>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting TF interactive session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow uses C++ to do its computation in the backend. This is because it is highly efficient and faster than python. Thus, this connection of Tensorflow in python to its backend C++ functions is known as Session. \n",
    "\n",
    "Generally, we first make a data flow graph in tensorflow and then launch it in a session, passing it the required placeholders or just evaluating it.\n",
    "\n",
    "We use InteractiveSession class which allows us to interleave/parellelize(allows us to make changes even after running the session) the operations which build a computational graph with one that runs the graph otherwise, we have to build the entire the computational graph before starting the session and launching/firing the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use lbraries like Numpy in python which does extensive numerical computation ouside python using some other languages(like C++). But this switching of values causes overhead processing which is bad for computations on GPUs or in a distributed manner computation due to high transferring costs.\n",
    "\n",
    "Tensorflow also computes externally but instead of running a single expensive operation independentlyfrom python, it lets us describe a graph of interacting operations that run entirely outside python which avoids this overhead cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a regression with a single linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x is the input and y_ is the labels. These are the nodes in the computational graph for input images and target output classes/one hot labels. x and y_ are values which we we'll input when we ask tensorflow to run a computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST has images of shape 28X28 which has been flattened to one dimension, hence having 784 input features represented by each pixel. The first dimension represents the batch size here. None presents a sence of variability and indicates that the batch can be of any size.\n",
    "The target output/labels, y_, will also consist of a 2d tensor where each row will be a 10 dimensional vector. The output classes inthe dataset ranges from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and biases here are the variables. Unlike placeholders, they do not interact with the outside world. Rather, they are those values that \"lives in tensorflow's computational graph\". They can be used and modified by computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Variable() method creates the variables and initializes them(0s here) W is a 784X10matrix(weight matrix shape= 784 inputs and 10 outputs{single layered}) and b is 10 dimensional vector as we have 10 classes(10 neurons in the single layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))#input to output dimensions of the weight matrix\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the variables can be used in a session, they need to be initialized in the session with their initial value which can be done using sess.run(variable)command. To initialize all of them at once, we use tf.global_variables_initializer() function to point to all the variables. It returns an Output that initializes global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted class and loss(cost) function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores in the final layer without applying softmax are called logit. We provide it the output of the previous layer without applying softmax to it(z) as its input and it returns the cross entropy as its output. \n",
    "\n",
    "tf,reduce_mean averages all the elements of a tensor, and hence calculates the mean cost over all the images.\n",
    "\n",
    "tf.nn.softmax_cross_entropy_with_logits Computes softmax cross entropy between `logits` and `labels`. Measures the probability error in discrete classification tasks in which the\n",
    "classes are mutually exclusive. It returns  a 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n",
    "\n",
    "In other words tf.nn.softmax_cross_entropy_with_logits internally applies the softmax on the model's unnormalized model prediction and sums across all classes, and tf.reduce_mean takes the average over these sums.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-78b4c0e0d208>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model/ The Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Tensorflow knows the wholle computational graph, it can use automatic differentiation to find the gradients(delL,dell,dcdw,dcdb). Here , this uses a gradient descent optimzer and trains the model according to minimizing the cross entropy(cost) of the whole system. It does so by adding operators in the model flow graph to compute cost gradients(delL, dell, dc/dw,dc/db, to compute the parameter updates-> wl=wl-alpha*(dc/dw) and bl=bl-alpha*(dc/db). \n",
    "Also, alpha here is learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha= tf.Variable(0.5)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(alpha).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?tf.train.GradientDescentOptimizer().minimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned operation train_step, when run, will apply the gradient descent updates to the parameters. Training the model can therefore be accomplished by repeatedly running train_step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The for loop here represents iterations we are performing. Here, we load a number of training examples in each iteration using mnist.train.next_batch(100) and store this value in batch. The 0the position in batch consists of the datapoints/images and the 1st positions in batch consists of the corresponding labels/target classes. This is fed to the placeholder x and y_ from batch using the feed_dict() function. \n",
    "\n",
    "We finally run the train step function in the session. Running any step in the session requires feeding of the placeholders. All the operations required to get to the point which we ran in the graph, are first executed. Hence, in a way, the whole graph gets executed one by one till the backwardpass itself. It feeds the input to x and labels to y_.First of all, the weights and biases gets initialized as forward pass requires them. The forward pass runs then, computing the output which is required by the cost computing function. According to this, the cross entropy is calculated as it is needed by the backward pass to minimize, and hence, the final step, train_step is run.\n",
    "\n",
    "The whole process is repeated several times according to the for loop argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.argmax is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. We can use tf.equal to check if our prediction matches the truth. This gives a array of booleans whose sum could be use to calculate total number of true positives. Casting this gives our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as soon as we run our .eval function of any valriable or stp in the model graph, it runs all the preceeding steps reuired to compute that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9167\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print((x.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions are used to initialize the weights an the biases of our CNN. Weights and Biases are non 0(with a small of noise). This is so as for symmetry breaking and 0 gradients. Also they ar positive since we use ReLU activated neurons which might be considered dead with negative weights.\n",
    "\n",
    "Both weights and biases are added as variables using tf.variable function.\n",
    "\n",
    "Truncated.normal creates a 0 mean 0.1 standard deviation gaussian distributed random values which is clipped after 0.2 from both sides.\n",
    "\n",
    "constant 0.1 biases are added. \n",
    "\n",
    "Shape is passed as parameter to these functions, hence the weights and biases arrays take the shape which we give to them while calling the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)#truncated, anything outside -0.2 and 0.2 has a probability of occurence 0\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)#constant array of 0.1 with whatever shape we give to it\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conv2d` function uses `tf.nn.conv2d` function to tell our convolutional parameters. It computes a 2D convoluted output from a 4Dinput applying the filter. The parameters of the function are the input image x tensor(which is a 4D tensor of shape `[batch_size, input_height,input_width, input_channels])`, W defines the filter(also called as kernel)tensor which has the shape `[filter_height, filter_width,Input_channels,Output_channels]`, stride size in all 4 dimensions, padding of 0s on all sides of the original image to make its dimensions favourable for our processing. \n",
    "It returns a `Tensor`. Has the same type as `input`.\n",
    "\n",
    "`tf.nn.max_pool`: Performs the max pooling on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.nn.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?tf.nn.max_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the weights and biases of our first conolutional layer which are nothing but the filter parameters which we want to convolve through our image. \n",
    "`W_conv1` is now the first filter weight tensor. The filter is a 5x5 filter with 1 channel depth( grayscale images are used only here). We need 32 random such filters to be applied on the image so as to obtain a feature map of depth 32.\n",
    "`b_conv1` are the bias variables attached to these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])#5X5 filter 1 is the in channel (gray-scale) and there are 32 feature maps\n",
    "b_conv1 = bias_variable([32])#32 feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape the grayscale 3D image toa 4D image by reshaping, adding a 1 as the 4th dimension (this is because 1 is not present in grayscale images hence making the dimensions of the image to be 3 rather than 4)\n",
    "-1 here acts as an 4th dimension alpha which must compute out to be a whole number (it should be equal to batch size for correct answers actually) otherwise it is a an error. This ensures that the image is corectly used.\n",
    "`x_image` represents the image here of the shapr 28X28, gray scale, hence 1 and the feature map contains 32 individual layers(whole number) hence -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?tf.reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply our image, `x_image` by the filter we just made by defining its weights, add the defined biases to it. This makes a feature map of depth 32 for our image. We then activate this output feature map using ReLU function. We convolute `x_image` and `W_conv1` using the function `conv2d`, add the biases `b_conv1` and apply the ReLU activation using the function `tf.nn.relu`\n",
    "\n",
    "Next, we apply 2X2 maxpooling function over the produced output which converts the feature map to the shape `14X14X1X32`. This is done using `max_pool_2x2` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)#64X28X28X32. Here 64 is the batch size (assumed)\n",
    "h_pool1 = max_pool_2x2(h_conv1)#max pooling 64X14X14X32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has 64 features in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])#64X14X14X64\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)#64X7X7X64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons to allow processing on the entire image.\n",
    "Thus, the outputs are mapped to 1024 neurons which learn using the backpropagation along with the weights of the filters we are applying to the images.\n",
    "This fully connected layer has a weight and bias given by `W_fc1` (dimensions `7X7` as the feature map, `64` input layers and `1024` output) and `b_fc1` \n",
    "\n",
    "We reshape the tensor we got from the last(2nd) maxpooling layer into a batch of vectors which is 2D and of dimensions of a whole number to be valid(1024) and `7*7*64`\n",
    "It the multiply the weight matrix with the input(features from the feature map transformed to a 1D represention using the `matmul` function. It adds the biases and then applies the `ReLU` activation function to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce overfitting, we will apply dropout before the readout layer. We create a placeholder for the probability that a neuron's output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing. TensorFlow's `tf.nn.dropout` output automatically handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)#probability of retaining the neurons. 0=all neurons dropped, 1 means no neuron is dropped\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReadOut Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add a layer, just like for the one layer softmax regression above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.06\n",
      "step 100, training accuracy 0.15\n",
      "step 200, training accuracy 0.22\n",
      "step 300, training accuracy 0.27\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:#declaring tf.Session as sess\n",
    "    sess.run(tf.global_variables_initializer())#initializing all the variables we used\n",
    "    for i in range(1000):#number of iterations we are using\n",
    "        batch = mnist.train.next_batch(100) #taking the slice of input from the mnist dataset\n",
    "        if i % 100 == 0:#check accuracy after every such(100) iterations\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})#training accuracy calculated with no dropout probabilty\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})#dropout probabilty during training is kept 0.5\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))#test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shalinigakhar7@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
