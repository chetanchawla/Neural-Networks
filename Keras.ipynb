{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense means a fully connected layer\n",
    "add adds a layer to the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data.\n",
    "Base_dir is the path where the data is stored and ANN or LSTM is the kind of data which is stored in npz format using np.savez\n",
    "It returns the training data, labels and validation data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(base_dir, kind):\n",
    "    \"\"\"\n",
    "    kind -  ANN / LSTM\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import os\n",
    "    data = np.load(os.path.join(base_dir, \"%s.npz\"%kind))\n",
    "    train_x, train_y = data[\"train_x\"], data[\"train_y\"]\n",
    "    val_x, val_y = data[\"val_x\"], data[\"val_y\"]\n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"C:/Users/asd/Desktop/Winter Training/BootcampStudents/Data/Task_1\"\n",
    "train_data,train_label,validate_data,validate_label=load_data(path,\"ANN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(588, 102400)\n",
      "2\n",
      "(58, 102400)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_data.ndim)\n",
    "print(validate_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "denseInput is the number of inputs to the first dense layer \\n\n",
    "neuronLayer1 is the number of neurons in the first hidden layer \\n\n",
    "drop prob is the probability of dropping the neurons in training \\n\n",
    "classes are the total classes at output or the number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseInput=102400\n",
    "neuronLayer1=128\n",
    "dropProb=0.3\n",
    "classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential() #We define a model which is sequential in nature.\n",
    "model.add(Dense(neuronLayer1, input_shape=(denseInput,)))#We can add layers to our model using model.add function\n",
    "#Dense(hidden) layer 1 us added with 128 neurons present in it and has the (denseInput,) number of inputs in it.\n",
    "# model.add(Activation('relu'))#\n",
    "model.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "#Then we do a batch normaization. The parameters are default.\n",
    "model.add(keras.layers.LeakyReLU(alpha=0.3))#We then apply a leakyReLU activation\n",
    "model.add(keras.layers.Dropout(dropProb, noise_shape=None, seed=None))#Then we apply a Dropout layer with dropping probability of 0.3.\n",
    "model.add(Dense(classes, activation='softmax'))#Then we add a final output dense layer having 5 neurons(the same as classes). We apply softmax activation to the final layer\n",
    "#model.add(softmax(x, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the model with adam optimizer. The loss is cross entropy format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model.fit fits the data to the compiled model graph and in a way does the woek of feed dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 588 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "588/588 [==============================] - 13s 22ms/step - loss: 0.4526 - acc: 0.8214 - val_loss: 4.6975 - val_acc: 0.5517\n",
      "Epoch 2/10\n",
      "588/588 [==============================] - 8s 14ms/step - loss: 0.1638 - acc: 0.9473 - val_loss: 1.6766 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "588/588 [==============================] - 9s 15ms/step - loss: 0.1159 - acc: 0.9762 - val_loss: 1.3249 - val_acc: 0.7241\n",
      "Epoch 4/10\n",
      "588/588 [==============================] - 8s 14ms/step - loss: 0.0704 - acc: 0.9864 - val_loss: 1.1055 - val_acc: 0.7069\n",
      "Epoch 5/10\n",
      "588/588 [==============================] - 8s 14ms/step - loss: 0.0584 - acc: 0.9898 - val_loss: 0.6810 - val_acc: 0.7931\n",
      "Epoch 6/10\n",
      "588/588 [==============================] - 8s 14ms/step - loss: 0.0380 - acc: 0.9983 - val_loss: 0.5160 - val_acc: 0.8103\n",
      "Epoch 7/10\n",
      "588/588 [==============================] - 8s 14ms/step - loss: 0.0424 - acc: 0.9881 - val_loss: 0.5115 - val_acc: 0.7931\n",
      "Epoch 8/10\n",
      "588/588 [==============================] - 8s 14ms/step - loss: 0.0259 - acc: 0.9966 - val_loss: 0.4207 - val_acc: 0.8793\n",
      "Epoch 9/10\n",
      "588/588 [==============================] - 8s 14ms/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.3321 - val_acc: 0.8793\n",
      "Epoch 10/10\n",
      "588/588 [==============================] - 8s 14ms/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.3708 - val_acc: 0.8793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b00ebe34a8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_label,validation_data=(validate_data,validate_label), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               13107328  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 13,108,485\n",
      "Trainable params: 13,108,229\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?keras.layers.BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?model.add(dense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rtrain_data,Rtrain_label,Rvalidate_data,Rvalidate_label=load_data(path,\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(588, 50, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(Rtrain_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(keras.layers.SimpleRNN(128, input_shape=(50,2048)))\n",
    "model1.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "model1.add(keras.layers.Dropout(dropProb, noise_shape=None, seed=None))\n",
    "model1.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 128)               278656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 279,813\n",
      "Trainable params: 279,557\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 588 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "588/588 [==============================] - 6s 9ms/step - loss: 0.9071 - acc: 0.6837 - val_loss: 0.7404 - val_acc: 0.7241\n",
      "Epoch 2/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.5437 - acc: 0.8231 - val_loss: 0.5516 - val_acc: 0.7759\n",
      "Epoch 3/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.4059 - acc: 0.8980 - val_loss: 0.7411 - val_acc: 0.6897\n",
      "Epoch 4/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.3592 - acc: 0.8963 - val_loss: 0.5848 - val_acc: 0.7586\n",
      "Epoch 5/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.2943 - acc: 0.9490 - val_loss: 0.4862 - val_acc: 0.8103\n",
      "Epoch 6/10\n",
      "588/588 [==============================] - 4s 7ms/step - loss: 0.2536 - acc: 0.9388 - val_loss: 0.4325 - val_acc: 0.8793\n",
      "Epoch 7/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.2132 - acc: 0.9507 - val_loss: 0.4856 - val_acc: 0.7241\n",
      "Epoch 8/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.2055 - acc: 0.9473 - val_loss: 0.4603 - val_acc: 0.8276\n",
      "Epoch 9/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.1761 - acc: 0.9541 - val_loss: 0.3555 - val_acc: 0.8793\n",
      "Epoch 10/10\n",
      "588/588 [==============================] - 3s 6ms/step - loss: 0.1640 - acc: 0.9694 - val_loss: 0.3591 - val_acc: 0.8966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b01eed82b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(Rtrain_data, Rtrain_label,validation_data=(Rvalidate_data,Rvalidate_label), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?keras.layers.SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ltrain_data,Ltrain_label,Lvalidate_data,Lvalidate_label=load_data(path,\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(588, 50, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(Ltrain_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(keras.layers.LSTM(128, input_shape=(50,2048)))\n",
    "model2.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "model2.add(keras.layers.Dropout(dropProb, noise_shape=None, seed=None))\n",
    "model2.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 588 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.1457 - acc: 0.9728 - val_loss: 0.5187 - val_acc: 0.7931\n",
      "Epoch 2/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.1275 - acc: 0.9677 - val_loss: 0.5594 - val_acc: 0.7414\n",
      "Epoch 3/10\n",
      "588/588 [==============================] - 3s 6ms/step - loss: 0.1302 - acc: 0.9711 - val_loss: 0.6280 - val_acc: 0.7759\n",
      "Epoch 4/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.1384 - acc: 0.9711 - val_loss: 0.8363 - val_acc: 0.6552\n",
      "Epoch 5/10\n",
      "588/588 [==============================] - 4s 7ms/step - loss: 0.1575 - acc: 0.9728 - val_loss: 0.8040 - val_acc: 0.7069\n",
      "Epoch 6/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.1446 - acc: 0.9558 - val_loss: 0.4823 - val_acc: 0.7931\n",
      "Epoch 7/10\n",
      "588/588 [==============================] - 4s 7ms/step - loss: 0.1660 - acc: 0.9507 - val_loss: 0.9068 - val_acc: 0.7069\n",
      "Epoch 8/10\n",
      "588/588 [==============================] - 4s 7ms/step - loss: 0.1361 - acc: 0.9609 - val_loss: 0.9205 - val_acc: 0.6724\n",
      "Epoch 9/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.1369 - acc: 0.9813 - val_loss: 1.0348 - val_acc: 0.6897\n",
      "Epoch 10/10\n",
      "588/588 [==============================] - 4s 6ms/step - loss: 0.1169 - acc: 0.9915 - val_loss: 0.7437 - val_acc: 0.7241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17986320240>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(Rtrain_data, Rtrain_label,validation_data=(Rvalidate_data,Rvalidate_label), epochs=10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
